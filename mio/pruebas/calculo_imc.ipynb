{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e0d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a991170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device\n",
    "use_gpu = True\n",
    "device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac016ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40.2416) tensor(156.0824)\n",
      "tensor(1.2000) tensor(2.3802)\n",
      "tensor(6.4576) tensor(91.5933)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_data(n=500, sigma=2, min_weight=45, max_weight=150, min_height=1.3, max_height=2):\n",
    "    \"\"\"\n",
    "    It is a function that gives you a tensor with the size, weight and imc of a person.\n",
    "\n",
    "    args:\n",
    "        n: number of tuples that you want\n",
    "        sigma: the standar variation of the values\n",
    "    \"\"\"\n",
    "    weight = torch.rand(n) * (max_weight - min_weight) + min_weight\n",
    "    height = torch.rand(n) * (max_height - min_height ) + min_height\n",
    "    imc = weight / torch.pow(height,2)\n",
    "\n",
    "    noise_weight = torch.normal(torch.zeros_like(weight), torch.ones_like(weight)*sigma)\n",
    "    noise_height = torch.normal(torch.zeros_like(weight), torch.ones_like(weight)*sigma * 0.05)\n",
    "    noise_imc = torch.normal(torch.zeros_like(weight), torch.ones_like(weight)*sigma)\n",
    "    \n",
    "    height = torch.clamp(height + noise_height, min=1.2)\n",
    "\n",
    "    return weight+noise_weight,height,imc+noise_imc\n",
    "\n",
    "\n",
    "class Persons_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, weight, height, imc, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.x = torch.stack([weight, height], dim=1).to(device)\n",
    "        self.y = imc.unsqueeze(1).to(device)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "weight, height, imc = get_data(50000)\n",
    "\n",
    "train_dataset = Persons_Dataset(weight,height,imc,device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dc58f",
   "metadata": {},
   "source": [
    "i will split the datasets in 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a97fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, split_share):\n",
    "    mask_indices_to_first_subset = torch.rand(len(dataset))<=split_share\n",
    "    indices_first_subset = [i for i, (_, _) in enumerate(dataset) if mask_indices_to_first_subset[i]]\n",
    "    indices_second_subset = [i for i, (_, _) in enumerate(dataset) if not mask_indices_to_first_subset[i]]\n",
    "\n",
    "\n",
    "    first_subset = torch.utils.data.Subset(dataset, indices_first_subset)\n",
    "    second_subset = torch.utils.data.Subset(dataset, indices_second_subset)\n",
    "\n",
    "    return first_subset, second_subset\n",
    "def split_datasets(train_dataset, test_dataset_share=0.3,val_dataset_share=0.2):\n",
    "    \"\"\"\n",
    "    This function it's used for split a dataset in three\n",
    "    args:\n",
    "        train_dataset: the initial dataset\n",
    "        test_dataset_share: the percentage of data that will have the test_datset\n",
    "        val_dataset_share: the percentage of data that will have the val_dataset\n",
    "    \"\"\"\n",
    "\n",
    "    first_subset, second_subset = split_dataset(train_dataset,test_dataset_share+val_dataset_share)\n",
    "    second_subset, thrid_subset = split_dataset(second_subset,val_dataset_share/(test_dataset_share+val_dataset_share))\n",
    "    return first_subset, second_subset, thrid_subset\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = split_datasets(train_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bdc297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Early_Stop():\n",
    "  def __init__(self, patience=5, delta=0):\n",
    "    self.patience = patience\n",
    "    self.delta = delta\n",
    "    self.best_val_loss = None\n",
    "    self.no_val_improvement_times = 0\n",
    "    self.stop = False\n",
    "\n",
    "  def check_stop(self, val_loss):\n",
    "    if self.best_val_loss is None or (val_loss + self.delta) < self.best_val_loss:\n",
    "      self.best_val_loss = val_loss\n",
    "      self.no_val_improvement_times = 0\n",
    "    else:\n",
    "      self.no_val_improvement_times += 1\n",
    "      self.stop = self.no_val_improvement_times >= self.patience\n",
    "      \n",
    "def learning_loop_for_regression_with_early_stopping(train_dataloader, val_dataloader, model, epochs, loss_fn, learning_rate, optimizer, validation_freq, early_stop):\n",
    "    epoch_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    opt = optimizer(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    with tqdm(range(epochs), desc=\"epoch:\") as pbar:\n",
    "        for epoch in pbar:\n",
    "            model.train()\n",
    "            steps_loss_list = []\n",
    "\n",
    "            for x_true, y_true in train_dataloader:\n",
    "                y_pred = model(x_true)\n",
    "                opt.zero_grad()\n",
    "                loss = loss_fn(y_pred, y_true)\n",
    "                loss.backward()\n",
    "                steps_loss_list.append(loss.item())\n",
    "                opt.step()\n",
    "\n",
    "            train_loss = sum(steps_loss_list)/len(steps_loss_list)\n",
    "            epoch_loss_list.append(train_loss)\n",
    "\n",
    "            # Validación solo cada `validation_freq` epochs\n",
    "            if epoch % validation_freq == 0:\n",
    "                model.eval()\n",
    "                val_step_loss_list = []\n",
    "                with torch.no_grad():\n",
    "                    for x_val_true, y_val_true in val_dataloader:\n",
    "                        y_val_pred = model(x_val_true)\n",
    "                        loss_val = loss_fn(y_val_pred, y_val_true)\n",
    "                        val_step_loss_list.append(loss_val.item())\n",
    "\n",
    "                val_loss = sum(val_step_loss_list)/len(val_step_loss_list)\n",
    "                val_loss_list.append(val_loss)\n",
    "                early_stop.check_stop(val_loss)\n",
    "                if early_stop.stop:\n",
    "                    pbar.set_postfix(train_loss=f\"{train_loss:.4f}\", val_loss=f\"{val_loss:.4f}\")\n",
    "                    print(\"Early Stop.\")\n",
    "                    break\n",
    "            else:\n",
    "                val_loss = val_loss_list[-1] if val_loss_list else None\n",
    "\n",
    "            # Actualizamos tqdm con train y val loss (val_loss puede ser None)\n",
    "            postfix = {\"train_loss\": f\"{train_loss:.4f}\"}\n",
    "            if val_loss is not None:\n",
    "                postfix[\"val_loss\"] = f\"{val_loss:.4f}\"\n",
    "            pbar.set_postfix(postfix)\n",
    "\n",
    "    return model, epoch_loss_list, val_loss_list\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9360cfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros 2305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch::   5%|▍         | 50/1001 [00:30<09:41,  1.64it/s, train_loss=3.8972, val_loss=3.8314]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "epochs = 1001\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam\n",
    "validation_freq = 5\n",
    "patience = 5\n",
    "delta = 0.01\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,64),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(64,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,1)\n",
    ").to(device)\n",
    "\n",
    "early_stop = Early_Stop(patience = patience,\n",
    "                        delta = delta)\n",
    "\n",
    "print(f\"Número de parámetros {torch.tensor([p.numel() for p in model.parameters()]).sum()}\")\n",
    "\n",
    "model, train_loss_list, val_loss_list = learning_loop_for_regression_with_early_stopping(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model,\n",
    "    epochs,\n",
    "    loss_fn,\n",
    "    learning_rate,\n",
    "    optimizer,\n",
    "    validation_freq = validation_freq,\n",
    "    early_stop = early_stop\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
