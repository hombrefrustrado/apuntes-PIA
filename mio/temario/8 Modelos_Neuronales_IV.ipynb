{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "bmGAVcmG7mXi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKHZUcUw0D0U"
      },
      "source": [
        "# Modelos Neuronales IV\n",
        "\n",
        "> Autor: Jorge García González (Universidad de Málaga)\n",
        "\n",
        "> Última Actualización: 9/10/2025\n",
        "\n",
        "> Asignatura: Programación para la Inteligencia Artificial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG1rXjSM9X3I"
      },
      "source": [
        "En el cuaderno anterior hemos hablado de cómo definir en PyTorch un modelo neuronal de varias capas usando **torch.nn.Sequential()**. Antes de continuar con más cuestiones teóricas es importante que hablemos de algunas utilidades relacionadas con los modelos neuronales en PyTorch. **torch.nn.Sequential()** es una manera muy útil de definir modelos a partir de capas predefinidas, pero en ocasiones necesitamos definir ajustables más complicados que simplemente aplicar capas y activaciones de manera secuencial. PyTorch ofrece una manera más flexible de definir nuestros modelos: simplemente definirlos como una clase nueva con el comportamiento que queramos. Vamos a ver un ejemplo en el que creamos un modelo de PyTorch para modelar una recta como hemos hecho en cuadernos anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "HiyLWAboEI0M"
      },
      "outputs": [],
      "source": [
        "class Line_2D_Model_PyTorch(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()              # Llamamos al constructor de la clase de la que heredamos por si tuviera algo que inicializar.\n",
        "    self.m = Parameter(torch.rand(1))\n",
        "    self.b = Parameter(torch.tensor(0).float())\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.m*x+self.b\n",
        "\n",
        "  def extra_repr(self):\n",
        "    return f\"inclinación={float(self.m.clone().detach())}, ordenada en el origen={float(self.b.clone().detach())}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS6Jn2E2O9qf"
      },
      "source": [
        "Como podemos ver, definimos una clase que hereda de **torch.nn.Module** y definimos sus parámetros optimizables como atributos de la clase indicados con **torch.nn.parameter.Parameter**. Luego definimos cómo opera este módulo su paso hacia delante con el método **forward**. El método **extra_repr** es simplemente una utilidad para definir qué mostrará la clase en un **print**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgctlNcfFut-",
        "outputId": "4eb36dbe-afc2-4a4f-fd99-de941f095026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Line_2D_Model_PyTorch(inclinación=0.07441467046737671, ordenada en el origen=0.0)\n",
            "[Parameter containing:\n",
            "tensor([0.0744], requires_grad=True), Parameter containing:\n",
            "tensor(0., requires_grad=True)]\n",
            "tensor([0.7441], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "line_2d_model = Line_2D_Model_PyTorch()\n",
        "print(line_2d_model)\n",
        "print([p for p in line_2d_model.parameters()])\n",
        "x=10\n",
        "print(line_2d_model(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANB0s9IKO8dg"
      },
      "source": [
        "Poder definir el constructor y el *forward* nos da libertad prácticamente absoluta para definir nuestros modelos siempre que durante el *forward* utilicemos operaciones que sean diferenciables para PyTorch (o no podrá realizar el *backward*).\n",
        "\n",
        "Ahora vamos a ver un ejemplo un poco más complejo en el que definimos un MLP, pero de una manera más personalizada que usando el modelo secuencial de PYTorch. Asumiremos que somos demasiado perezosos para definir las capas línea por línea y queremos automatizarlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "dv7ZQ0tkA_SE"
      },
      "outputs": [],
      "source": [
        "class My_MLP(torch.nn.Module):\n",
        "  def __init__(self, list_of_neurons, list_of_activations, device='cpu'):\n",
        "    super().__init__()         # Llamamos al constructor de la clase de la que heredamos por si tuviera algo que inicializar.\n",
        "    self.list_of_layers = []\n",
        "    self.list_of_activations = list_of_activations\n",
        "\n",
        "    assert len(list_of_neurons)==len(list_of_activations)           # Sanity Check.\n",
        "\n",
        "    for n in list_of_neurons:\n",
        "      if len(self.list_of_layers)==0:\n",
        "        self.list_of_layers.append(nn.LazyLinear(n).to(device))\n",
        "      else:\n",
        "        self.list_of_layers.append(nn.Linear(previous_n, n).to(device))\n",
        "      previous_n=n\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    outputs = None\n",
        "    for layer, activation in zip(self.list_of_layers, self.list_of_activations):\n",
        "      outputs = activation(layer(inputs)) if outputs is None else activation(layer(outputs))\n",
        "    return outputs\n",
        "\n",
        "  def extra_repr(self):\n",
        "    return f\"{[p for p in self.parameters()]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjuG3ksPTnmA"
      },
      "source": [
        "Es una lógica sencilla en la que inicializamos la instancia comprobando que la longitud de la lista con las neuronas por capa y el número de funciones de activación coincide (**assert** comprueba que una condición se cumple y para la ejecución con un error en caso contrario) y luego inicializamos todas las capas lineales de PyTorch (recordemos que no incluyen activación) con el número indicado. A la hora de hacer el paso hacia adelante simplemente vamos ejecutando capa a capa con su correspondiente función de activación.\n",
        "\n",
        "Hay un detalle interesante en este código: el uso de LazyLinear para la primera capa. LazyLinear simplemente es una capa Lineal \"vaga\" que no requiere indicar el tamaño de la entrada (más información: https://docs.pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear)\n",
        "\n",
        "Vamos a probarlo..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOEzaJcETs1M",
        "outputId": "be9861b0-bc17-454f-da5f-08897e059ef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3, 5, 1]\n",
            "[ReLU(), ReLU(), ReLU()]\n",
            "My_MLP([])\n"
          ]
        }
      ],
      "source": [
        "list_of_neurons = [3,5,1]\n",
        "list_of_activations = 3*[torch.nn.ReLU()]\n",
        "mlp = My_MLP(list_of_neurons, list_of_activations)\n",
        "print(list_of_neurons)\n",
        "print(list_of_activations)\n",
        "print(mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIxG42IRYk1E"
      },
      "source": [
        "Ups. Hemos definido que muestre los parámetros del MLP cuando hacemos un print, sin embargo, la lista de parámetros está vacía. Esto indcia que PyTorch no reconoce ningún parámetro optimizable. Problemático. ¿No podemos entonces usar otros objetos **torch.nn.Module** (que es lo que son los **torch.nn.Linear**) al crear nuestra propia **torch.nn.Module** clase? Hagamos otra prueba suponiendo una clase que solo funciona para MLPs con dos capas ocultas más la capa de salida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "Z6a30JXYZYTX"
      },
      "outputs": [],
      "source": [
        "class My_MLP(torch.nn.Module):\n",
        "  def __init__(self, list_of_neurons, list_of_activations, device='cpu'):\n",
        "    super().__init__()         # Llamamos al constructor de la clase de la que heredamos por si tuviera algo que inicializar.\n",
        "    self.layer1 = nn.LazyLinear(list_of_neurons[0]).to(device)\n",
        "    self.layer2 = nn.Linear(list_of_neurons[0], list_of_neurons[1]).to(device)\n",
        "    self.layer3 = nn.Linear(list_of_neurons[1], list_of_neurons[2]).to(device)\n",
        "    self.activation1 = list_of_activations[0]\n",
        "    self.activation2 = list_of_activations[1]\n",
        "    self.activation3 = list_of_activations[2]\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    outputs = self.activation1(self.layer1(inputs))\n",
        "    outputs = self.activation2(self.layer2(outputs))\n",
        "    outputs = self.activation3(self.layer3(outputs))\n",
        "    return outputs\n",
        "\n",
        "  def extra_repr(self):\n",
        "    return f\"{[p for p in self.parameters()]}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpr6vuOJZ-GV",
        "outputId": "014f1738-2282-49a1-cb90-636d1c4d1dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3, 5, 1]\n",
            "[ReLU(), ReLU(), ReLU()]\n",
            "My_MLP(\n",
            "  [<UninitializedParameter>, <UninitializedParameter>, Parameter containing:\n",
            "  tensor([[ 0.2323, -0.3665, -0.1739],\n",
            "          [-0.0948,  0.0778,  0.3858],\n",
            "          [-0.0061, -0.1340, -0.2615],\n",
            "          [-0.3173,  0.1205, -0.5644],\n",
            "          [-0.5141, -0.2040, -0.4174]], requires_grad=True), Parameter containing:\n",
            "  tensor([ 0.2320, -0.3404,  0.1516,  0.0970, -0.2009], requires_grad=True), Parameter containing:\n",
            "  tensor([[-0.3354,  0.1948,  0.2246,  0.4458,  0.2878]], requires_grad=True), Parameter containing:\n",
            "  tensor([-0.2756], requires_grad=True)]\n",
            "  (layer1): LazyLinear(in_features=0, out_features=3, bias=True)\n",
            "  (layer2): Linear(in_features=3, out_features=5, bias=True)\n",
            "  (layer3): Linear(in_features=5, out_features=1, bias=True)\n",
            "  (activation1): ReLU()\n",
            "  (activation2): ReLU()\n",
            "  (activation3): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "list_of_neurons = [3,5,1]\n",
        "list_of_activations = 3*[torch.nn.ReLU()]\n",
        "mlp = My_MLP(list_of_neurons, list_of_activations)\n",
        "print(list_of_neurons)\n",
        "print(list_of_activations)\n",
        "print(mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfv-ooqMcXe4"
      },
      "source": [
        "Interesante. Esta vez sí obtenemos información al hacer un **print** y parece que, además de los parámetros como le hemos indicado, noes muestra de manera ordenada las capas y las funciones de activación que hay en nuestro modelo. ¿Cuál era el problema con un modelo con un número de capas flexible? Pues un tecnicismo de PyTorch. Si usamos un modelo como sub-modelo, PyTorch espera que sea en un atributo de orden superior (que no esté dentro de una estructura de datos como una lista o un diccionario). Para poder usar listas o diccionarios sin que PyTorch pase por alto esos sub-modulos hay que usar las clases especiales **torch.nn.ModuleList** y **torch.nn.ModuleDict**. Vamos a verlo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "5Kmqvqwd-KoY"
      },
      "outputs": [],
      "source": [
        "class My_MLP(torch.nn.Module):\n",
        "  def __init__(self, list_of_neurons, list_of_activations, device='cpu'):\n",
        "    super().__init__()         # Llamamos al constructor de la clase de la que heredamos por si tuviera algo que inicializar.\n",
        "    self.list_of_layers = nn.ModuleList()\n",
        "    self.list_of_activations = nn.ModuleList(list_of_activations)\n",
        "\n",
        "    assert len(list_of_neurons)==len(list_of_activations)           # Sanity Check.\n",
        "\n",
        "    for n in list_of_neurons:\n",
        "      if len(self.list_of_layers)==0:\n",
        "        self.list_of_layers.append(nn.LazyLinear(n).to(device))\n",
        "      else:\n",
        "        self.list_of_layers.append(nn.Linear(previous_n, n).to(device))\n",
        "      previous_n=n\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    print(inputs.shape)\n",
        "    outputs = None\n",
        "    for layer, activation in zip(self.list_of_layers, self.list_of_activations):\n",
        "      outputs = activation(layer(inputs)) if outputs is None else activation(layer(outputs))\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGJ_0u3E-X5w",
        "outputId": "c9a73cc9-39c1-4afa-dc44-0db3ff6750fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3, 5, 1]\n",
            "[ReLU(), ReLU(), ReLU()]\n",
            "My_MLP(\n",
            "  (list_of_layers): ModuleList(\n",
            "    (0): LazyLinear(in_features=0, out_features=3, bias=True)\n",
            "    (1): Linear(in_features=3, out_features=5, bias=True)\n",
            "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
            "  )\n",
            "  (list_of_activations): ModuleList(\n",
            "    (0-2): 3 x ReLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "list_of_neurons = [3,5,1]\n",
        "list_of_activations = 3*[torch.nn.ReLU()]\n",
        "mlp = My_MLP(list_of_neurons, list_of_activations)\n",
        "print(list_of_neurons)\n",
        "print(list_of_activations)\n",
        "print(mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbyfSk9cWijz"
      },
      "source": [
        "Hagamos una inferencia rápida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXCJVZcLWk6G",
        "outputId": "7db0da68-a7eb-4c9f-8636-947a57cd8002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1])\n"
          ]
        }
      ],
      "source": [
        "y = mlp(torch.tensor([[10]]).float())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yBuNBk_Zff7"
      },
      "source": [
        "El resultado de la inferencia da igual, lo importante es que la podemos hacer y que si observamos ahora mlp..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smiKHgxfXB0O",
        "outputId": "eb5c9d5d-1e08-4c47-9e2d-0d968ae58849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My_MLP(\n",
            "  (list_of_layers): ModuleList(\n",
            "    (0): Linear(in_features=1, out_features=3, bias=True)\n",
            "    (1): Linear(in_features=3, out_features=5, bias=True)\n",
            "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
            "  )\n",
            "  (list_of_activations): ModuleList(\n",
            "    (0-2): 3 x ReLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbTcQvhbZmLk"
      },
      "source": [
        "¡Ya no hay capa **LazyLinear**! En su lugar hay una capa **Linear** con una única entrada. Deduce el tamaño de la entrada durante el primer paso de **forward**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2snuxxRw-hp4"
      },
      "source": [
        "Ya funciona como debe. Es importantísimo que PyTorch detecte cuáles son los parámetros optimizables del modelo o el método **parameters()** no podrá devolverlos y hay que porporcionarselos al optimizador para que pueda realizarse el ajuste!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeAl0ls_BJhn"
      },
      "source": [
        "Antes de pasar al siguiente tema, es importante señalar un detalle. Hasta la fecha, siempre que hemos usado un modelo de PyTorch para hacer el paso hacia delante (*forward*), lo hemos llamado con una instrucción del tipo **model(x)**. Como si fuera un método, aunque el modelo es un objeto. Por defecto, esa instrucción lo que hace es llamar al método **\\_\\_call__()**, un método heredado que no hace falta definir en la subclase. Sería lo mismo hacer **model.\\_\\_call__(x)**. Uno podría pensar que en vez de hacerlo así, se podría hacer **model.forward(x)**, ya que definimos el método **forward()** para la subclase. Sin embargo, aunque **\\_\\_call__()** hace una llamada a **forward()**, no es lo único que hace. **\\_\\_call__()** realiza cálculos extra que PyTorch necesita para su buen funcionamiento y llamar directamente a **forward()** podría llevar a comportamientos indeseados. Aunque definamos el **forward()**, la llamada para hacer un *forward* siempre debe ser a través del **\\_\\_call__()**. Además es más sencillo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2X14KzrE4HB"
      },
      "source": [
        "Ahora toca hablar de otra cosa puramente práctica, pero importante. ¿Cómo guardamos un modelo una vez entrenado?\n",
        "\n",
        "PyTorch ofrece distintas alternaetivas para guardar diversos elementos según lo que se desea hacer después con esos modelos. El uso más intuitivo de un modelo una vez entrenado es usarlo para hacer inferencia (predecir respuestas para datos nuevos). Sin embargo, uno también puede querer guardar un modelo para continuar con el entrenamiento más tarde o para permitir hacer *fine-tuning* (de eso ya hablaremos más adelante).\n",
        "\n",
        "En este tutorial se comentan las diversas maneras de guardar y cargar un modelo según lo que se desea poder hacer después: https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "Nosotros vamos a comentar aquí la más básica y directa, pero para casi todas hay que entender el concepto de **state_dict**. ¿Qué es? Como su nombre indica, es un diccionario que define el estado de algo en PyTorch. Ese algo puede ser un modelo, pero también hay otros elementos con **state_dict** (como los optimizadores). Veamos el del modelo anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFmrloaIWCjy",
        "outputId": "bacc9ff2-5f8a-4862-c520-f34198ce59de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict({'list_of_layers.0.weight': tensor([[ 0.0145],\n",
            "        [ 0.9210],\n",
            "        [-0.0457]]), 'list_of_layers.0.bias': tensor([ 0.5404, -0.4528,  0.3808]), 'list_of_layers.1.weight': tensor([[-0.4683, -0.2743,  0.0049],\n",
            "        [ 0.3161, -0.1513, -0.2963],\n",
            "        [ 0.0941,  0.3087,  0.1365],\n",
            "        [ 0.2028, -0.2521, -0.1802],\n",
            "        [-0.3079,  0.5393, -0.1272]]), 'list_of_layers.1.bias': tensor([ 0.4349, -0.0700,  0.3118, -0.0064, -0.3699]), 'list_of_layers.2.weight': tensor([[ 0.4012, -0.1140,  0.2599, -0.2850,  0.1847]]), 'list_of_layers.2.bias': tensor([0.3774])})\n"
          ]
        }
      ],
      "source": [
        "print(mlp.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqb0uPpSWNk-"
      },
      "source": [
        "Como se puede ver, el **state_dict** no es más que una forma estructurada de devolver los parámetros optimizables asociándolos a las capas. Si queremos guardar un modelo, lo que hacemos es guardar ese state dict. Vamos a hacer una pequeña prueba guardando el modelo anterior y cargándolo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKrNgy6HabMM",
        "outputId": "fd953998-79b8-4337-8c0b-50df9a18ca58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "workpath = '/content/drive/MyDrive/Work/Docencia UMA/2025-2026/Programacion para la IA/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVBtVInYa0Lk",
        "outputId": "34e02d0a-fe13-4778-da8b-83e7b6410667"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.save(mlp.state_dict(), os.path.join(workpath, 'mlp.pt'))\n",
        "mlp2 = My_MLP(list_of_neurons, list_of_activations)\n",
        "mlp2.load_state_dict(torch.load(os.path.join(workpath, 'mlp.pt'), weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjHwyhJUclTJ"
      },
      "source": [
        "Todo bien. Vamos a echar un vistazo a la estructura de capas de ambos y a sus **state_dict**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFmBMyNRb7-x",
        "outputId": "2c8d7f3f-e11c-45a2-d7b3-7e19e3971452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My_MLP(\n",
            "  (list_of_layers): ModuleList(\n",
            "    (0): Linear(in_features=1, out_features=3, bias=True)\n",
            "    (1): Linear(in_features=3, out_features=5, bias=True)\n",
            "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
            "  )\n",
            "  (list_of_activations): ModuleList(\n",
            "    (0-2): 3 x ReLU()\n",
            "  )\n",
            ")\n",
            "OrderedDict({'list_of_layers.0.weight': tensor([[ 0.0145],\n",
            "        [ 0.9210],\n",
            "        [-0.0457]]), 'list_of_layers.0.bias': tensor([ 0.5404, -0.4528,  0.3808]), 'list_of_layers.1.weight': tensor([[-0.4683, -0.2743,  0.0049],\n",
            "        [ 0.3161, -0.1513, -0.2963],\n",
            "        [ 0.0941,  0.3087,  0.1365],\n",
            "        [ 0.2028, -0.2521, -0.1802],\n",
            "        [-0.3079,  0.5393, -0.1272]]), 'list_of_layers.1.bias': tensor([ 0.4349, -0.0700,  0.3118, -0.0064, -0.3699]), 'list_of_layers.2.weight': tensor([[ 0.4012, -0.1140,  0.2599, -0.2850,  0.1847]]), 'list_of_layers.2.bias': tensor([0.3774])})\n",
            "My_MLP(\n",
            "  (list_of_layers): ModuleList(\n",
            "    (0): LazyLinear(in_features=0, out_features=3, bias=True)\n",
            "    (1): Linear(in_features=3, out_features=5, bias=True)\n",
            "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
            "  )\n",
            "  (list_of_activations): ModuleList(\n",
            "    (0-2): 3 x ReLU()\n",
            "  )\n",
            ")\n",
            "OrderedDict({'list_of_layers.0.weight': tensor([[ 0.0145],\n",
            "        [ 0.9210],\n",
            "        [-0.0457]]), 'list_of_layers.0.bias': tensor([ 0.5404, -0.4528,  0.3808]), 'list_of_layers.1.weight': tensor([[-0.4683, -0.2743,  0.0049],\n",
            "        [ 0.3161, -0.1513, -0.2963],\n",
            "        [ 0.0941,  0.3087,  0.1365],\n",
            "        [ 0.2028, -0.2521, -0.1802],\n",
            "        [-0.3079,  0.5393, -0.1272]]), 'list_of_layers.1.bias': tensor([ 0.4349, -0.0700,  0.3118, -0.0064, -0.3699]), 'list_of_layers.2.weight': tensor([[ 0.4012, -0.1140,  0.2599, -0.2850,  0.1847]]), 'list_of_layers.2.bias': tensor([0.3774])})\n"
          ]
        }
      ],
      "source": [
        "print(mlp)\n",
        "print(mlp.state_dict())\n",
        "print(mlp2)\n",
        "print(mlp2.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "AKVAyjUMcSwY",
        "outputId": "c469ca57-36c7-4253-f48c-0eb6c49ec74b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2])\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "The in_features inferred from input: 2 is not equal to in_features from self.weight: 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-683350750.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2035331528.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_of_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_of_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1880\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1804\u001b[0m                 ):\n\u001b[1;32m   1805\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks_with_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1806\u001b[0;31m                         \u001b[0margs_kwargs_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1807\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0margs_kwargs_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_kwargs_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_kwargs_result\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/lazy.py\u001b[0m in \u001b[0;36m_infer_parameters\u001b[0;34m(self, module, args, kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \"\"\"\n\u001b[1;32m    260\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_uninitialized_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'module {self._get_name()} has not been fully initialized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36minitialize_parameters\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             assert input.shape[-1] == self.weight.shape[-1], (\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0;34mf\"The in_features inferred from input: {input.shape[-1]} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;34mf\"is not equal to in_features from self.weight: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: The in_features inferred from input: 2 is not equal to in_features from self.weight: 1"
          ]
        }
      ],
      "source": [
        "y = mlp2(torch.tensor([[10,10]]).float())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ySriK-cigv"
      },
      "source": [
        "Y... error. Vale, sí, hemos ido a buscar el error. Pero es un buen momento para reflexionar sobre lo que estamos haciendo. Solo hemos cargado el **state_dict** en otra instancia del modelo, por lo tanto necesitamos que esa isntancia exista. ¡Necesitamos tener ya un código que monte la misma estructura de capas y lo único que hemos guardado son los parámetros en esa estructura de capas.\n",
        "\n",
        "Pero claro, teníamos una capa **LazyLinear**. No nos ha dado problema al cargarle la información de una capa **Linear**, pero al intentar hacer un paso de *forward* con un tamaño de entrada distinto al de los parámetros que le hemos dado, no ha podido hacer coincidir las matrices y ha fallado. Si lo hacemos sin mala fe debería funcionar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12pLckMwg5K0",
        "outputId": "61354c10-871b-4644-f773-63072453f08f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp2 = My_MLP(list_of_neurons, list_of_activations)\n",
        "mlp2.load_state_dict(torch.load(os.path.join(workpath, 'mlp.pt'), weights_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gdqMYjlg8l3",
        "outputId": "78b837ba-4e83-4114-ba0c-1c3d0a07d713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1])\n"
          ]
        }
      ],
      "source": [
        "y = mlp2(torch.tensor([[10]]).float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIgzOuMIg-l8",
        "outputId": "2f223158-6685-49ee-e670-a744bd74cd33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My_MLP(\n",
            "  (list_of_layers): ModuleList(\n",
            "    (0): Linear(in_features=1, out_features=3, bias=True)\n",
            "    (1): Linear(in_features=3, out_features=5, bias=True)\n",
            "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
            "  )\n",
            "  (list_of_activations): ModuleList(\n",
            "    (0-2): 3 x ReLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH0ige70hJtD"
      },
      "source": [
        "Todo se ha cargado como debía y nuestra **LazyLinear** se ha convertido satisfactoriamente en una capa **Linear**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmxoE1Rb7p1M"
      },
      "source": [
        "Dada una neurona lineal, sabemos que tiene n+1 parámetros (un parámetro para cada entrada más el bias). Si una capa tiene tiene k neuronas lineales, esa capa tendrá k(n+1)=kn+k parámetros.\n",
        "\n",
        "Supongamos un perceptrón multicapa con una capa oculta con k<sub>1</sub> neuronas y una neurona en la capa de salida. Para n entradas, la red tendrá k<sub>1</sub>(n+1)+k<sub>1</sub>+1 = k<sub>1</sub>(n+2)+1 parámetros.\n",
        "\n",
        "Ahora supongamos un segundo perceptrón con dos capas ocultas de k<sub>1</sub> y k<sub>2</sub> neuronas y una capa de salida de una única neurona. La red tendrá k<sub>1</sub>(n+1)+k<sub>2</sub>(k<sub>1</sub>+1)+k<sub>2</sub>+1 = k<sub>1</sub>(n+1)+k<sub>2</sub>k<sub>1</sub>+2k<sub>2</sub>+1 parámetros.\n",
        "\n",
        "Supongamos que queremos disponer 10 neuronas en las capas ocultas y la entrada es un único valor. En el primer caso, tendríamos 10(1+2)+1=31 parámetros. Si tenemos dos capas ocultas de 5 neuronas, en el segundo caso tendremos 5(2)+5(5)+2(5)+1=10+25+10+1=46 parámetros.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQJiLXzb-ZpH",
        "outputId": "880520c7-158b-4541-e618-4f2ea9b7b97e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(31)\n",
            "tensor(46)\n"
          ]
        }
      ],
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(1,10),\n",
        "    torch.nn.Linear(10,1),\n",
        ")\n",
        "print(torch.tensor([p.numel() for p in model.parameters()]).sum())\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(1,5),\n",
        "    torch.nn.Linear(5,5),\n",
        "    torch.nn.Linear(5,1),\n",
        ")\n",
        "print(torch.tensor([p.numel() for p in model.parameters()]).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpWJC4sM-cEo"
      },
      "source": [
        "¿Qué pasaría si en vez de 1 valor de entrada hubiera 10? En el primer caso los números cambiarían a 10(12)+1 = 121 parámetros, mientras que en el segundo se elevaría a 5(11)+5(6)+5+1=91."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ew-Opbx-dkx",
        "outputId": "d59ec01c-5038-42dd-fda8-bc44d8aee5b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(121)\n",
            "tensor(91)\n"
          ]
        }
      ],
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(10,10),\n",
        "    torch.nn.Linear(10,1),\n",
        ")\n",
        "print(torch.tensor([p.numel() for p in model.parameters()]).sum())\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(10,5),\n",
        "    torch.nn.Linear(5,5),\n",
        "    torch.nn.Linear(5,1),\n",
        ")\n",
        "print(torch.tensor([p.numel() for p in model.parameters()]).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqD75SHT-fnZ"
      },
      "source": [
        "Tener una única capa oculta hace que los parámetros crezcan más rápido en relación al tamaño de la entrada que si hacemos redes más profundas. Las redes más profundas además dotan al modelo de mayor flexibilidad a menor coste. Intuitivamente añadir capas es componer funciones. Incluso con el mismo número de neuronas.\n",
        "\n",
        "y=f<sub>1</sub>(f<sub>2</sub>(f<sub>3</sub>(f<sub>4</sub>(x))))\n",
        "\n",
        "Cada función aplica una transformación que la siguiente puede \"aprovechar\". Si hablaramos de neuronas convolucionales diríamos que cada capa aprende características que el siguiente nivel puede componer enc aracterísticas de más alto nivel.\n",
        "\n",
        "En la práctica, elegir la arquitectura de la red atiende a unos criterios estrictos de limitación física relativos a la memoria: más parámetros requieren más espacio en memoria y provocan entrenamientos más lentos. Además, redes más profundas también provocan entrenamientos más lentos de por sí.\n",
        "\n",
        "Recordemos que, aunque hasta ahora no lo hayamos hecho, las redes se entrenan típicamente en GPUs en las que se busca maximizar el paralelismo, pero cada capa secuencial tiene que esperar al resultado de la capa anterior para poder computarse, con lo que se está forzando un cómputo en secuencia en lugar de en paralelo.\n",
        "\n",
        "Hay un equilibrio que al final responde a la prueba y el error, el hardware disponible y el problema."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
